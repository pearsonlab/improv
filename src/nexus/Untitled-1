#!/usr/bin/env python
# coding: utf-8


import numpy as np
import torch
from PIL import Image
import numpy as np
import matplotlib as plt
import os
from IPython.display import display
import math
import torchvision
from torchvision import datasets, transforms, models
import torch.optim as optim
import time
import torch.nn as nn
import torch.nn.functional as F


def imshow(img):
    image= img.detach().numpy()
    image= np.moveaxis(image, 0, -1)
    im= Image.fromarray(np.uint8(image))
    display(im)



class SeparableConv2d(nn.Module):
    def __init__(self,in_channels,out_channels,kernel_size=1,stride=1,padding=0,dilation=1,bias=False):
        super(SeparableConv2d,self).__init__()

        self.conv1 = nn.Conv2d(in_channels,in_channels,kernel_size,stride,padding,dilation,groups=in_channels,bias=bias)
        self.pointwise = nn.Conv2d(in_channels,out_channels,1,1,0,1,1,bias=bias)
    
    def forward(self,x):
        x = self.conv1(x)
        x = self.pointwise(x)
        return x


def getFile(folder, image):

    cwd= os.getcwd()
    filepath = "../../../usr/project/xtmp/bbp13"
    blur= filepath+ '/train/train_blur'
    sharp= filepath+ '/train/train_sharp'

    fileblur= blur+'/'+folder+'/'+image+'.png'
    filesharp= sharp+'/'+folder+'/'+image+'.png'

    #print(fileblur)

    imblur=Image.open(fileblur)
    imsharp= Image.open(filesharp)

    #display(imblur)
    #display(imsharp)
    
    np_imblur= np.array(imblur)
    np_imsharp= np.array(imsharp)
    
    return np_imblur, np_imsharp



class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.F0 = SeparableConv2d(15, 64, 5, padding=2) 
        self.D1 = SeparableConv2d(64, 64, 3, stride=2, padding=1)
        self.F1_1 = SeparableConv2d(64, 128, 3, padding=1)
        self.F1_2 = SeparableConv2d(128, 128, 3, padding=1)
        self.D2 = SeparableConv2d(128, 256, 3, stride= 2, padding=1)
        self.F2_1 = SeparableConv2d(256, 256, 3, padding=1)
        self.F2_2 = SeparableConv2d(256, 256, 3, padding=1)
        self.F2_3 = SeparableConv2d(256, 256, 3, padding=1)
        self.D3 = SeparableConv2d(256,  512, 3, stride=2, padding=1)
        self.F3_1 = SeparableConv2d(512, 512, 3, padding=1)
        self.F3_2 = SeparableConv2d(512, 512, 3, padding=1)
        self.F3_3 = SeparableConv2d(512, 512, 3, padding=1)
        self.U1 = nn.ConvTranspose2d(512, 256, 4, stride=2, padding=1)
        self.F4_1 = SeparableConv2d(256, 256, 3, padding=1)
        self.F4_2 = SeparableConv2d(256, 256, 3, padding=1)
        self.F4_3 = SeparableConv2d(256, 256, 3, padding=1)
        self.U2 = nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1)
        self.F5_1 = SeparableConv2d(128, 128, 3, padding=1)
        self.F5_2 = SeparableConv2d(128, 64, 3, padding=1)
        self.U3 = nn.ConvTranspose2d(64, 64, 4, stride=2, padding=1)
        self.F6_1 = SeparableConv2d(64, 15, 3, padding=1)
        self.F6_2 = SeparableConv2d(15, 3, 3, padding=1)
        
        self.batch15 = nn.BatchNorm2d(15)
        self.batch64 = nn.BatchNorm2d(64)
        self.batch128 = nn.BatchNorm2d(128)
        self.batch256 = nn.BatchNorm2d(256)
        self.batch512 = nn.BatchNorm2d(512)
        self.batch3 = nn.BatchNorm2d(3)
        

    def forward(self, inp):
        skip1= self.batch15(inp)
        x= F.relu(skip1)
        skip2= self.batch64(self.F0(x))
        x= F.relu(skip2)
        x= F.relu(self.batch64(self.D1(x)))
        x= F.relu(self.batch128(self.F1_1(x)))
        skip3= self.batch128(self.F1_2(x))
        x= F.relu(skip3)
        x= F.relu(self.batch256(self.D2(x)))
        x= F.relu(self.batch256(self.F2_1(x)))
        x= F.relu(self.batch256(self.F2_2(x)))
        skip4= self.batch256(self.F2_3(x))
        x= F.relu(skip4)
        x= F.relu(self.batch512(self.D3(x)))
        x= F.relu(self.batch512(self.F3_1(x)))
        x= F.relu(self.batch512(self.F3_2(x)))
        x= F.relu(self.batch512(self.F3_3(x)))
        x= F.relu(self.batch256(self.U1(x))+skip4)
        x= F.relu(self.batch256(self.F4_1(x)))
        x= F.relu(self.batch256(self.F4_2(x)))
        x= F.relu(self.batch256(self.F4_3(x)))
        x= F.relu(self.batch128(self.U2(x))+skip3)
        x= F.relu(self.batch128(self.F5_1(x)))
        x= F.relu(self.batch64(self.F5_2(x)))
        x= F.relu(self.batch64(self.U3(x))+skip2)
        x= F.relu(self.batch15(self.F6_1(x))+skip1)
        x= F.relu(self.batch3(self.F6_2(x)))
      
        return x



def weights_init_uniform(m):
    classname = m.__class__.__name__
    # for every Linear layer in a model..
    if classname.find('Linear') != -1:
        # apply a uniform distribution to the weights and a bias=0
        m.weight.data.uniform_(0.0, 1.0)
        m.bias.data.fill_(0)



def get_batch(batch_size):

    batchblur= np.zeros((batch_size, 15, 720, 1280))
    batchsharp= np.zeros((batch_size, 3, 720, 1280))
    for j in range(batch_size):
        folder= np.random.randint(30)
        if folder>9:
            folder= '0'+str(folder)
        else:
            folder= '00'+str(folder)
          
        pic= np.random.randint(100)

        currimbl= np.zeros((720, 1280, 15))
        currimsh= np.zeros((720, 1280, 3))

        for k in range(5):
            curr= pic-2+k

            if curr==-2:
                curr=0
            if curr==-1:
                curr=0
            if curr==100:
                curr= 99
            if curr== 101:
                curr= 99
            if curr>9:
                get_pict= '000000'+str(curr)
            else:   
                 get_pict= '0000000'+str(curr)

            imbl, imsh= getFile(folder, get_pict)
            
            currimbl[:,:,3*k:3*k+3]= imbl
            if k==2:
                currimsh[:,:,:]= imsh
            
            
        currimbl= np.moveaxis(currimbl, -1, 0)
        currimsh= np.moveaxis(currimsh, -1, 0)
             
        batchblur[j,:,:,:]= currimbl
        batchsharp[j,:,:,:]= currimsh
        
    return batchblur, batchsharp


def train_net(num_epochs, batch_size):

    net= Net()
    net.apply(weights_init_uniform)
    net.to(cuda)
    lr = .005
    optimizer = optim.Adam(net.parameters(), lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)
    for i in range(num_epochs):
        if((i < 116 and i > 30) and (i % 12 == 8)) or ((i > 106 ) and (i % 18 == 0)):
            lr = lr/2
            optimizer = optim.Adam(net.parameters(), lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)
        blur, sharp = get_batch(batch_size)
        blurBatch= torch.tensor(blur, device=cuda).float()
        sharpBatch= torch.tensor(sharp, device=cuda).float()
        
        out = net(blurBatch)
        target = sharpBatch
        criterion = nn.MSELoss()
        optimizer.zero_grad()   
        loss = criterion(out, target)
        print(loss)
        loss.backward()
        optimizer.step()    
        
    torch.save(net.state_dict(), 'Baseline_500_4_1.pt')


if __name__ == '__main__':
    cuda = torch.device('cuda')
    start_time = time.time()
    train_net(1, 5)
    print("%s" % (time.time()-start_time))




